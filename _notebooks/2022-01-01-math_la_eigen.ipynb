{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9603489-e5dc-4d85-84f3-2425d1441775",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Eigenvectors and Eigenvalues\n",
    "> Recently I'm reviewing mathematic basis which I think would be useful for the the machine learning. In this post, I am going to show the definition and properties of eigenvector and eigenvalue.\n",
    "\n",
    "- toc: true\n",
    "- comments: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- categories: [Math]\n",
    "- image: images/la_eigen.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679369c-590a-481b-ac89-2197d0ecc56e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Definition\n",
    "Based on [wikipedia](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors), in linear algebra, an eigenvector or characteristic vector of a linear transformation is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. The corresponding eigenvalue, often denoted by $\\lambda$, is the factor by which the eigenvector is scaled.\n",
    "\n",
    "It's a bit difficult to understand. Let's translate it. Suppose we have a matrix $A\\in{R^{m\\times n}}$, vector $v_1$ is one of its eigenvector. Then we have $Av_1=\\lambda v_1$. We know $Av_1$ is the linear transformation process applied to the vector $v_1$ with the transformer $A$. The direction of the original vector $v_1$ is not changed but just scaled by $\\lambda$.\n",
    "\n",
    "In a nutshell, the eigenvector-eigenvalue pairs tell us in which direction is a vector distorted after the linear transformation.\n",
    "\n",
    "![image.png](./2022-01-01-math_la_eigen_files/att_00000.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a13cdc-3cdf-464a-abbc-cf4223239c6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Diagonalization and Eigendecomposition\n",
    "\n",
    "We know $Av_1=\\lambda v_1$, supporse $V=[v_1,v_2,...,v_d]$ and $\\lambda_1, \\lambda_2,..., \\lambda_d$ are the eigenvectors and eigenvalues of the matrix $A$. Then we can write the following equation based on the property of eigenvectors and eigenvalues.\n",
    "\n",
    "$AV = V\\Lambda$, where $\\Lambda \\in R^{d*d}$ is the diagonal matrix with eigenvalues filled in the diagonal line.\n",
    "\n",
    "- **Diagonalization**: multiply by $V^{-1}$ at both left sides we get $V^{-1}AV = V^{-1}V\\Lambda = \\Lambda$\n",
    "- **Eigendecomposition**: multiply $V^{-1}$ at both right sides we get $A = V\\Lambda V^{-1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185d2a9-f151-4341-b7d9-d7c1786206b3",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Eigenvectors and eigenvalues are widely used in machine learning mode. For instance, in PCA(Pricipal Component Analysis) it is used to decompose covariance matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33c63a-1ded-4f3a-a1a5-308f57cc4867",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Sho Nakagome. (2018). [Linear Algebra â€” Part 6: eigenvalues and eigenvectors](https://medium.com/sho-jp/linear-algebra-part-6-eigenvalues-and-eigenvectors-35365dc4365a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
