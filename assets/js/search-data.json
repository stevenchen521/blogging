{
  
    
        "post0": {
            "title": "Positive Definiteness",
            "content": "Definition . Based on wikipedia, in mathematics, a symmetric matrix $M$ with real entries is positive-definite if the real number $z^TMz$ is positive for every nonzero real column vector $z$, where $z^T$ is the transpose of $z$ . Geomatric interpretation . Suppose we have matrix $M in R^{m times n}$ and vector $z in R^n$. We can transform $z$ by $ hat{z} = Mz$. After the transformation the direction of $ hat z$ is changed(excpet if $z$ is the eigenvector of $M$) compared to $z$. The angel between them is within 90 degrees we call the matrix $M$ is the positive definite matrix. . . Proposition . Continue with the above definition, some propositions can be extended. . Positive definitive matrix is full rank or invertable . Proof: suppose $M$ is not full rank, then its columns are not linearly independent. Therefore, there exists some non-trivial vector $v in R^{n times 1}$ such that . $Mv = 0$ . We multiple the left and right terms by $v^T$, we have . $v^TMv = M parallel v parallel^2 = 0$ . Since $M$ is positive definite matrix, the vector $v$ has to be trivial. This is contradicted to the assumption. . Positive definitive matrix has positive eigenvalues . Proof: matrix $M$ is positive definite, then we have . $Mz = lambda z$ Then, $z^TMz = lambda parallel{z} parallel^2 &gt; 0$ . Therefore, $ lambda$ which are the eigen values of $M$ have to be positive. . Summary . In this post I listed a few bullets about the positive definity. I think the definition and the geomatric interpretation are quite useful for the machine learning. In the next posts, I am going to continue the linear algebra topics. . Reference . Sho Nakagome. (2018). Linear Algebra 101 ‚Äî Part 8: Positive Definite Matrix . Aerin Kim(2019). What is a Positive Definite Matrix? . Taboga, Marco (2021). &quot;Positive definite matrix&quot;, Lectures on matrix algebra. .",
            "url": "https://stevenchen521.github.io/blogging/math/2021/12/27/math_la_pd.html",
            "relUrl": "/math/2021/12/27/math_la_pd.html",
            "date": " ‚Ä¢ Dec 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "AWS VPC Exercise",
            "content": "Overview . The following chart shows the VPC architecture we will build in this exercise . Prerequisite . The resource is from the freeCodeCamp.org presented by digitalcloud.training. Neal did a quite good job I encourage you to watch the original video and do the exercise by yourself. Here I only logged the steps what I have done. . Create a free AWS Account. | Download the exercise code | Install AWS Cli and config the aws configure | . VPC Review . VPC Components . Virtual Private Cloud (VPC): A logically isolated virtual network in the AWS cloud | Subnet: A segment of a VPC&#39;s IP address range where you can place groups of isolated resources | Internet Gateway/Egress only Internet Gateway: The Amazon VPC side of a connection to the public Internet for IPv4/IPv6 | Router: Routers interconnect subnets and direct traffic between Internet gateways, virtual private gateways, NAT gateways, and subnets | Peering Connection: Direct connection between two VPCs | VPC Endpoints: Private connection to public AWS services | NAT Instance: Enables Internet access for EC2 instances in private subnets (managed by you) | NAT Gateway: Enables Internet access for EC2 instances in private subnets (managed by AWS) | Virtual Private Gateway: The Amazon VPC side of a Virtual Private Network (VPN) connection | Customer Gateway: Customer side of a VPN connection | AWS Direct Connect: High speed, high bandwidth, private network connection from customer to aws | Security Group: Instance-level firewall | Network ACL: Subnet-level firewall | . Amazon VPC Core Knowledge . Difference between Internet Gateway and NAT Gateway | A virtual private cloud (VPC) is a virtual network dedicated to your AWS account | Analogous to having your own data center inside AWS | It is logically isolated from other virtual networks in the AWS Cloud | Provides complete control over the virtual networking environment including selection of IP ranges, creation of subnets, and configuration of route tables and gateways | You can launch your AWS resources, such as Amazon EC2 instances, into your VPC | When you create a VPC, you must specify a range of IPv4 addresses for the VPC in the form of a Classless Inter-Domain Routing (CIDR) block; for example, 10.0.0.0/16 | A VPC spans all the Availability Zones in the region | You have full control over who has access to the AWS resources inside your VPC | By default you can create up to 5 VPCs per region | A default VPC is created in each region with a subnet in each AZ | . Create VPC . Name: MyVPC IPv4 CIDR Block: 10.0.0.0/16 . . A route table was also created automatically by AWS. . Create Subnets . Name: Public-1A Availability Zone: us-east-1a IPv4 CIDR Block: 10.0.1.0/24 . Name: Public-1B Availability Zone: us-east-1b IPv4 CIDR Block: 10.0.2.0/24 . Name: Private-1A Availability Zone: us-east-1a IPv4 CIDR Block: 10.0.3.0/24 . Name: Private-1B Availability Zone: us-east-1b IPv4 CIDR Block: 10.0.4.0/24 . Finally, they look like this. . For the public subnets, we tick &#39;Enable auto-assign public IPv4 address&#39;. . . Create private route table . Name: Private-RT VPC: MyVPC Subnet associations: Private-1A, Private-1B . Change the name of default route table to &#39;MAIN&#39; and associate the Private-1A and Private-1B to the route table. . . Create Internet Gateway . Name: MyIGW VPC: MyVPC . . . Edit MAIN route table . Add 0.0.0.0/0 to internet gateway. . . . NAT Gateway . Create NAT Gateway | . Go to PrivateRT and edit the route 0.0.0.0/0 to NAT Gateway | . Configure Security Groups and NACLs . Create Public-Web security group | . Private-App: Private App Access, Inbound rule: http/80 with source Public-Web, make sure the inbound comes from the web app front end in the Public-Web security group | . . Launch EC2s . Use the following command to launch the EC2s. aws ec2 run-instances --image-id --instance-type --security-group-ids --subnet-id --key-name --user-data &lt;/strong&gt;&lt;/p&gt; The variables that need to be filled: . image-id: Amazon Machine Images (AMI), here we use ami-0ed9277fb7eb570c9 | instance-type: t2.micro | security-group-ids: Public-Web security group. | subnet-id: we create two EC2s in Public 1A and 1B, one EC2 in Private 1B. | key-name: key pairs of EC2. | user-data are needed to be filled: the file &#39;/user-data-subnet-id.txt&#39; in exercise code | . Once we have these ready we run the following script(Make sure the aws cli installed and configured before this). . Launch instance in Public 1A aws ec2 run-instances --image-id ami-0ed9277fb7eb570c9 --instance-type t2.micro --security-group-ids sg-0ff58fb3c21c0792d --subnet-id subnet-0f0f447e902559be9 --key-name ec2_cloud --user-data file://code/user-data-subnet-id.txt . | Launch instance in Public 1B aws ec2 run-instances --image-id ami-0ed9277fb7eb570c9 --instance-type t2.micro --security-group-ids sg-0ff58fb3c21c0792d --subnet-id subnet-07266ffa901687189 --key-name ec2_cloud --user-data file://code/user-data-subnet-id.txt . | Launch instance in Private 1B aws ec2 run-instances --image-id ami-0ed9277fb7eb570c9 --instance-type t2.micro --security-group-ids sg-0ff58fb3c21c0792d --subnet-id subnet-0f44825a48340db38 --key-name ec2_cloud --user-data file://code/user-data-subnet-id.txt . | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Finally three EC2 are launched. . . Visit the Instances in Public 1A and 1B. . We see the response from the server: &quot;This instance is in the subnet wih ID: subnet-0f0f447e902559be9&quot;. . . Change inbound rule of Public-Web security group, set the source to &quot;my ip&quot;. Then try it also with another IP(turn VPN on). Change it back after the experiment. . Once the VPN is turned on, the process bar was hardly moving which means the &quot;block&quot; was working. . Change it back after the testing. . Test the internal network with SSH. . Ping from Public 1A to Public 1B . . . Ping from Public 1A to Private 1B . . Change the security group of EC2 Private 1B to Private-App . . In the Private-App, only HTTP/80 was configured that Ping(ICMP) didn&#39;t work and CURL(Http) worked from Public 1A to Private 1B . Conclusion . I finished this exercise and it makes me understand the core concepts of VPC. Potentially I may go to get some certificates and hopefully I can plan a schedule for it. . &lt;/div&gt; .",
            "url": "https://stevenchen521.github.io/blogging/aws/2021/12/16/aws_vpc.html",
            "relUrl": "/aws/2021/12/16/aws_vpc.html",
            "date": " ‚Ä¢ Dec 16, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Upsert DataFrame to MongoDB",
            "content": "Requirement . Let&#39;s say we have a DataFrame. We want to &#39;upsert&#39;(Insert/update) the MongoDB collection with rows in DataFrame. More specifically, for the rows that don&#39;t exist we do the creation, otherwise the update should be performed. . def show_collection(collection): display(DataFrame([row for row in collection.find()])) . Setup Cloud MongoDB for Free . Setup a free cloud based MongoDB database here. . Connect to the database and create the &#39;test&#39; database and &#39;employee&#39; collection. . DB_NAME = &#39;test&#39; COLLECTION_NAME = &#39;employee&#39; . client = MongoClient(CONN_STR) client.drop_database(DB_NAME) # clear the database db = client[DB_NAME] # switch database collection = db[COLLECTION_NAME] # get the collection . now = datetime.now() emplyee = [[&#39;user1&#39;,25,&#39;male&#39;, now],[&#39;user2&#39;,55,&#39;male&#39;, now],[&#39;user3&#39;,43,&#39;male&#39;, now]] df_emplyee = DataFrame(emplyee, columns=[&#39;name&#39;,&#39;age&#39;,&#39;sex&#39;, &#39;lastModifiedAt&#39;]) df_emplyee = df_emplyee[[&#39;name&#39;,&#39;age&#39;,&#39;sex&#39;]] . Prepare a test DataFrame df_emplyee that contains three columns: . name | age | sex | . df_emplyee . name age sex . 0 user1 | 25 | male | . 1 user2 | 55 | male | . 2 user3 | 43 | male | . Once the records are inserted to the collection, we get the _id column for every row. &#39;_id&#39; column is a potentially a good shard key used by MongoDB cluster. . collection.insert_many(df_emplyee.to_dict(&quot;records&quot;)) show_collection(collection) . _id name age sex . 0 61bb536d771f7760addc5d1c | user1 | 25 | male | . 1 61bb536d771f7760addc5d1d | user2 | 55 | male | . 2 61bb536d771f7760addc5d1e | user3 | 43 | male | . Update . Let&#39;s do the update. Change the age of user1 to 32 with update_one method. . filter parameter get the rows that need update. | $set flag is to update the columns specified. | $currentDate here is to update lastModifiedAt column with the current time. | . myquery = { &quot;name&quot;: &quot;user1&quot; } newvalues = { &quot;$set&quot;: { &quot;age&quot;: &quot;32&quot; }, &quot;$currentDate&quot;: {&quot;lastModifiedAt&quot;: { &quot;$type&quot;: &quot;date&quot; }} } res = collection.update_one(filter=myquery, update=newvalues) . show_collection(collection) . _id name age sex lastModifiedAt . 0 61bb536d771f7760addc5d1c | user1 | 32 | male | 2021-12-16 14:55:41.593 | . 1 61bb536d771f7760addc5d1d | user2 | 55 | male | NaT | . 2 61bb536d771f7760addc5d1e | user3 | 43 | male | NaT | . Upsert . As the name suggests &#39;upsert&#39; means update/insert records based on the specified filters. For intansce, in the following first example, &#39;user1&#39; exists in the collection. The &#39;upsert&#39; performs the update. In the second example, user4 doesn&#39;t exist and it is inserted to the collection. . collection.update_one({&quot;name&quot;:&quot;user1&quot;}, {&quot;$set&quot;:{&quot;age&quot;:32}, &quot;$currentDate&quot;: {&quot;lastModifiedAt&quot;: { &quot;$type&quot;: &quot;date&quot; }} }, upsert=True) show_collection(collection) . _id name age sex lastModifiedAt . 0 61bb536d771f7760addc5d1c | user1 | 32 | male | 2021-12-16 14:55:41.807 | . 1 61bb536d771f7760addc5d1d | user2 | 55 | male | NaT | . 2 61bb536d771f7760addc5d1e | user3 | 43 | male | NaT | . collection.update_one({&quot;name&quot;:&quot;user4&quot;}, {&quot;$set&quot;:{&quot;age&quot;:32}, &quot;$setOnInsert&quot;:{&quot;sex&quot;:&quot;female&quot;}, &quot;$currentDate&quot;:{&quot;lastModifiedAt&quot;: { &quot;$type&quot;: &quot;date&quot; }} }, upsert=True) show_collection(collection) . _id name age sex lastModifiedAt . 0 61bb536d771f7760addc5d1c | user1 | 32 | male | 2021-12-16 14:55:41.807 | . 1 61bb536d771f7760addc5d1d | user2 | 55 | male | NaT | . 2 61bb536d771f7760addc5d1e | user3 | 43 | male | NaT | . 3 61bb536eef8c67586582a4ea | user4 | 32 | female | 2021-12-16 14:55:42.012 | . Bulk updates . Obviously it&#39;s inefficient if the updates/inserts are performed one by one. To improve this, we firstly collect all the &#39;UpdateOne&#39; operations and perform the updates with &#39;bulk_write&#39; like the following. . df_emplyee = df_emplyee.append({&#39;name&#39;:&#39;user5&#39;,&#39;age&#39;: 65, &#39;sex&#39;:&#39;male&#39;},ignore_index=True) updates = [] df_emplyee.apply( lambda row: updates.append( UpdateOne( {&quot;name&quot;: row.get(&quot;name&quot;)}, {&quot;$set&quot;: row.to_dict(), &quot;$currentDate&quot;:{&quot;lastModifiedAt&quot;: { &quot;$type&quot;: &quot;date&quot; }} }, upsert=True )), axis=1) collection.bulk_write(updates) show_collection(collection) . _id name age sex lastModifiedAt . 0 61bb536d771f7760addc5d1c | user1 | 25 | male | 2021-12-16 14:55:42.229 | . 1 61bb536d771f7760addc5d1d | user2 | 55 | male | 2021-12-16 14:55:42.230 | . 2 61bb536d771f7760addc5d1e | user3 | 43 | male | 2021-12-16 14:55:42.230 | . 3 61bb536eef8c67586582a4ea | user4 | 32 | female | 2021-12-16 14:55:42.012 | . 4 61bb536eef8c67586582a4f6 | user5 | 65 | male | 2021-12-16 14:55:42.230 | . DataFrame Upsert . Back to our initial requirement, we need to perform the &#39;Upsert&#39; on individual DataFrame row. I created a function df_upsert for this. . def df_upsert(df:DataFrame, collection, keys:[]): def row_query(row, keys ): res = {} for key in keys: res[key] = row.get(key) return res updates = [] df_emplyee.apply( lambda row: updates.append( UpdateOne( row_query(row, keys), {&#39;$set&#39;: row.to_dict(), &quot;$currentDate&quot;:{&quot;lastModifiedAt&quot;: { &quot;$type&quot;: &quot;date&quot; }} }, upsert=True)), axis=1 ) collection.bulk_write(updates) . Let&#39;s do some testing. Some updates are applied to the df_emplyee. Here are the difference between the df_emplyee and the MongoDB collection. As we see, user6 is added and user1&#39;s age is changed to 20 from 25. Please note user4 doesn&#39;t exist in the df_emplyee, it will not be touched in the collection. . df_emplyee.loc[0,&#39;age&#39;] = 20 df_emplyee = df_emplyee.append({&#39;name&#39;:&#39;user6&#39;,&#39;age&#39;: 37, &#39;sex&#39;:&#39;female&#39;},ignore_index=True) show_collection(collection) display(df_emplyee) . _id name age sex lastModifiedAt . 0 61bb536d771f7760addc5d1c | user1 | 25 | male | 2021-12-16 14:55:42.229 | . 1 61bb536d771f7760addc5d1d | user2 | 55 | male | 2021-12-16 14:55:42.230 | . 2 61bb536d771f7760addc5d1e | user3 | 43 | male | 2021-12-16 14:55:42.230 | . 3 61bb536eef8c67586582a4ea | user4 | 32 | female | 2021-12-16 14:55:42.012 | . 4 61bb536eef8c67586582a4f6 | user5 | 65 | male | 2021-12-16 14:55:42.230 | . name age sex . 0 user1 | 20 | male | . 1 user2 | 55 | male | . 2 user3 | 43 | male | . 3 user5 | 65 | male | . 4 user6 | 37 | female | . Once we perform the &#39;df_upsert&#39;, we get the expected results. . df_upsert(df_emplyee, collection, [&#39;name&#39;]) show_collection(collection) . _id name age sex lastModifiedAt . 0 61bb536d771f7760addc5d1c | user1 | 20 | male | 2021-12-16 14:55:42.574 | . 1 61bb536d771f7760addc5d1d | user2 | 55 | male | 2021-12-16 14:55:42.575 | . 2 61bb536d771f7760addc5d1e | user3 | 43 | male | 2021-12-16 14:55:42.575 | . 3 61bb536eef8c67586582a4ea | user4 | 32 | female | 2021-12-16 14:55:42.012 | . 4 61bb536eef8c67586582a4f6 | user5 | 65 | male | 2021-12-16 14:55:42.575 | . 5 61bb536eef8c67586582a51b | user6 | 37 | female | 2021-12-16 14:55:42.575 | . Consolution . In this post I showed some basic operations of MongoDB through Python and made the function df_upsert. The function df_upsert can be polished further, for instance we can do: . Pass a dictionary of mulitiple rows to the function with collection name as key and DataFrame as the value. | Upate strategy: drop &amp; creation or incremental updates? | . This is my first blog and thanks for your reading. .",
            "url": "https://stevenchen521.github.io/blogging/nosql/python/2021/12/09/mogodb_01.html",
            "relUrl": "/nosql/python/2021/12/09/mogodb_01.html",
            "date": " ‚Ä¢ Dec 9, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "üë§Ô∏è About Me",
          "content": "My name is Steven CHEN and I have been working for 12 years. I started from Java and then I shift to SAP. I got the master from City University Hong Kong major in Data Science. Currently I‚Äôm interested in Machine Learning(especially in Financial Market), DevOps and Software archecture(Micro Service). . My contact: Email .",
          "url": "https://stevenchen521.github.io/blogging/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://stevenchen521.github.io/blogging/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}